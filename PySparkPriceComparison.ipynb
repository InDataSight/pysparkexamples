{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+t60GRmnH07+ohAd1wqkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InDataSight/pysparkexamples/blob/main/PySparkPriceComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi8CZyqQVEmy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal is to compare different prices for the same component. Demonstrating the base logic what is process, what kind of data are being used. Also practice the pyspark basics.\n",
        "\n",
        "Note: All data used are publicly available. Price moves are dummy, to demostrate the price moves."
      ],
      "metadata": {
        "id": "V8VNkvbNVR5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3-scala2.13.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3-scala2.13.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bPcoasJVltu",
        "outputId": "daa2e8c1-3ae3-4061-fa51-f5eb38e10282"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 8,380 B/129 kB 6%] [Connecting to cloud.r\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,197 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,632 kB]\n",
            "Fetched 4,225 kB in 6s (756 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, col, expr\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PriceComparison\").getOrCreate()"
      ],
      "metadata": {
        "id": "m5lgO5qyV8Nw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "pyYFbTeGWaLs",
        "outputId": "d62f6d6a-ab70-438d-ca57-f70671fe1b13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e81a053a8c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b4ef26089d44:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PriceComparison</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.dell.com/en-us/shop/memory-upgrades/ar/8134/32gb?appliedRefinements=728\n",
        "#https://www.dell.com/en-us/shop/servers-storage-and-networking/poweredge-r660-rack-server/spd/poweredge-r660/pe_r660_vi_vp_sb_deals\n",
        "#https://www.dell.com/en-us/shop/memory-upgrades/ar/8134/3200mhz-and-above?appliedRefinements=32720,1311,728\n",
        "#https://www.cdw.com/search/servers-server-management/server-components/server-memory/?w=SA2&b=DLE&filter=af_rf_memory_memory_speed_ss%3a(%224800+megahertz%22)&filterClicked=Memory%20Speed\n",
        "#https://www.cdw.com/search/servers-server-management/server-components/server-memory/?lfr=1&w=SA2&key=P43328-B21\n",
        "#"
      ],
      "metadata": {
        "id": "fBdolwXX80ok"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_data = [\n",
        "    {\n",
        "        \"PartNo\": \"P43328-B21\",\n",
        "        \"description\": \"HPE 32GB (1x32GB) Dual Rank x8 DDR5-4800 CAS-40-39-39 EC8 Registered Smart Memory Kit\",\n",
        "        \"company\": \"HPE\"\n",
        "    },\n",
        "    {\n",
        "        \"PartNo\": \"AC830717\",\n",
        "        \"description\": \"Dell Memory Upgrade - 32 GB - 2Rx8 DDR5 RDIMM 5600 MT/s (Not Compatible with 4800 MT/s DIMMs)\",\n",
        "        \"company\": \"DELLUpgrades\"\n",
        "    },\n",
        "    {\n",
        "        \"PartNo\": \"370-12345\",\n",
        "        \"description\": \"32GB RDIMM, 5600MT/s, Dual Rank\",\n",
        "        \"company\": \"DELLServer\"\n",
        "    },\n",
        "    {\n",
        "        \"PartNo\": \"P64706-B21\",\n",
        "        \"description\": \"HPE 32GB (1x32GB) Dual Rank x8 DDR5-5600 CAS-46-45-45 EC8 Registered Smart Memory Kit\",\n",
        "        \"company\": \"HPE\"\n",
        "    },\n",
        "    {\n",
        "        \"PartNo\": \"AC239378\",\n",
        "        \"description\": \"Dell Memory Upgrade - 32 GB - 2Rx8 DDR5 RDIMM 4800 MT/s (Not Compatible with 5600 MT/s DIMMs)\",\n",
        "        \"company\": \"DELLUpgrades\"\n",
        "    },\n",
        "    {\n",
        "        \"PartNo\": \"370-12344\",\n",
        "        \"description\": \"32GB RDIMM, 4800MT/s, Dual Rank\",\n",
        "        \"company\": \"DELLServer\"\n",
        "    },\n",
        "]\n",
        "\n",
        "products_df = spark.createDataFrame(products_data)\n",
        "products_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLrH3uDbWg1Y",
        "outputId": "245e9492-c149-4f70-8b3e-e71a25064620"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+---------------------------------------------------------------------------------------------+\n",
            "|PartNo    |company     |description                                                                                  |\n",
            "+----------+------------+---------------------------------------------------------------------------------------------+\n",
            "|P43328-B21|HPE         |HPE 32GB (1x32GB) Dual Rank x8 DDR5-4800 CAS-40-39-39 EC8 Registered Smart Memory Kit        |\n",
            "|AC830717  |DELLUpgrades|Dell Memory Upgrade - 32 GB - 2Rx8 DDR5 RDIMM 5600 MT/s (Not Compatible with 4800 MT/s DIMMs)|\n",
            "|370-12345 |DELLServer  |32GB RDIMM, 5600MT/s, Dual Rank                                                              |\n",
            "|P64706-B21|HPE         |HPE 32GB (1x32GB) Dual Rank x8 DDR5-5600 CAS-46-45-45 EC8 Registered Smart Memory Kit        |\n",
            "|AC239378  |DELLUpgrades|Dell Memory Upgrade - 32 GB - 2Rx8 DDR5 RDIMM 4800 MT/s (Not Compatible with 5600 MT/s DIMMs)|\n",
            "|370-12344 |DELLServer  |32GB RDIMM, 4800MT/s, Dual Rank                                                              |\n",
            "+----------+------------+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attributes_data = [\n",
        "    {\"PartNo\": \"P43328-B21\", \"capacity\": \"32GB\", \"speed\": \"4800MTs\", \"rank\": \"DualRank\", \"technology\": \"RDIMM\"},\n",
        "    {\"PartNo\": \"P64706-B21\", \"capacity\": \"32GB\", \"speed\": \"5600MTs\", \"rank\": \"DualRank\", \"technology\": \"RDIMM\"},\n",
        "    {\"PartNo\": \"AC830717\", \"capacity\": \"32GB\", \"speed\": \"5600MTs\", \"rank\": \"DualRank\", \"technology\": \"RDIMM\"},\n",
        "    {\"PartNo\": \"370-12345\", \"capacity\": \"32GB\", \"speed\": \"5600MTs\", \"rank\": \"DualRank\", \"technology\": \"RDIMM\"},\n",
        "    {\"PartNo\": \"370-12344\", \"capacity\": \"32GB\", \"speed\": \"4800MTs\", \"rank\": \"DualRank\", \"technology\": \"RDIMM\"},\n",
        "    {\"PartNo\": \"AC239378\", \"capacity\": \"32GB\", \"speed\": \"4800MTs\", \"rank\": \"DualRank\", \"technology\": \"RDIMM\"}\n",
        "\n",
        "]\n",
        "\n",
        "attributes_df = spark.createDataFrame(attributes_data)\n",
        "attributes_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZNJIvr0Wn8p",
        "outputId": "cc5ea0cd-7c4e-444a-ce22-88516f569542"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+--------+-------+----------+\n",
            "|PartNo    |capacity|rank    |speed  |technology|\n",
            "+----------+--------+--------+-------+----------+\n",
            "|P43328-B21|32GB    |DualRank|4800MTs|RDIMM     |\n",
            "|P64706-B21|32GB    |DualRank|5600MTs|RDIMM     |\n",
            "|AC830717  |32GB    |DualRank|5600MTs|RDIMM     |\n",
            "|370-12345 |32GB    |DualRank|5600MTs|RDIMM     |\n",
            "|370-12344 |32GB    |DualRank|4800MTs|RDIMM     |\n",
            "|AC239378  |32GB    |DualRank|4800MTs|RDIMM     |\n",
            "+----------+--------+--------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "price_adjustments_data = [\n",
        "    {\"date\": \"2024-09-01\", \"adjustment_factor\": -0.05},  # -5%\n",
        "    {\"date\": \"2024-10-01\", \"adjustment_factor\": 0.00},   # baseline (0%)\n",
        "    {\"date\": \"2024-11-01\", \"adjustment_factor\": 0.07}    # +7%\n",
        "]\n",
        "\n",
        "adjustments_df = spark.createDataFrame(price_adjustments_data)\n",
        "adjustments_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NclGUrkhWyj7",
        "outputId": "a5e553df-1e52-4a3b-9c2a-50c4c2b3f347"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------+\n",
            "|adjustment_factor|      date|\n",
            "+-----------------+----------+\n",
            "|            -0.05|2024-09-01|\n",
            "|              0.0|2024-10-01|\n",
            "|             0.07|2024-11-01|\n",
            "+-----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_prices_data = [\n",
        "    {\"PartNo\": \"P43328-B21\", \"date\": \"2024-10-01\", \"list_price\": 2498.00, \"cdw_price\": 615.99},\n",
        "    {\"PartNo\": \"AC830717\", \"date\": \"2024-10-01\", \"list_price\": 1344.91, \"cdw_price\": None},\n",
        "    {\"PartNo\": \"370-12345\", \"date\": \"2024-10-01\", \"list_price\": 1399.28, \"cdw_price\": None},\n",
        "    {\"PartNo\": \"P64706-B21\", \"date\": \"2024-10-01\", \"list_price\": 2498.00, \"cdw_price\": 615.99},\n",
        "    {\"PartNo\": \"AC239378\", \"date\": \"2024-10-01\", \"list_price\": 1151.86, \"cdw_price\": 825.99},\n",
        "    {\"PartNo\": \"370-12344\", \"date\": \"2024-10-01\", \"list_price\": 1399.28, \"cdw_price\": None}\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "base_prices_df = spark.createDataFrame(base_prices_data)\n",
        "base_prices_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Ic_pUxW5po",
        "outputId": "8aa40b6b-2192-44c5-eff5-68216939ec50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+----------+\n",
            "|    PartNo|cdw_price|      date|list_price|\n",
            "+----------+---------+----------+----------+\n",
            "|P43328-B21|   615.99|2024-10-01|    2498.0|\n",
            "|  AC830717|     NULL|2024-10-01|   1344.91|\n",
            "| 370-12345|     NULL|2024-10-01|   1399.28|\n",
            "|P64706-B21|   615.99|2024-10-01|    2498.0|\n",
            "|  AC239378|   825.99|2024-10-01|   1151.86|\n",
            "| 370-12344|     NULL|2024-10-01|   1399.28|\n",
            "+----------+---------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_prices_data = [\n",
        "    {\"PartNo\": \"P43328-B21\", \"date\": \"2024-10-01\", \"list_price\": 2498.00, \"cdw_price\": 615.99},\n",
        "    {\"PartNo\": \"AC830717\", \"date\": \"2024-10-01\", \"list_price\": 1344.91, \"cdw_price\": None},\n",
        "    {\"PartNo\": \"370-12345\", \"date\": \"2024-10-01\", \"list_price\": 1399.28, \"cdw_price\": None},\n",
        "    {\"PartNo\": \"P64706-B21\", \"date\": \"2024-10-01\", \"list_price\": 2498.00, \"cdw_price\": 615.99},\n",
        "    {\"PartNo\": \"AC239378\", \"date\": \"2024-10-01\", \"list_price\": 1151.86, \"cdw_price\": 825.99},\n",
        "    {\"PartNo\": \"370-12344\", \"date\": \"2024-10-01\", \"list_price\": 1399.28, \"cdw_price\": None}\n",
        "]\n",
        "\n",
        "base_prices_df = spark.createDataFrame(base_prices_data)\n",
        "base_prices_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqSFxWglW9tU",
        "outputId": "ef464e11-1f28-4e13-95f7-693d81328672"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+----------+\n",
            "|    PartNo|cdw_price|      date|list_price|\n",
            "+----------+---------+----------+----------+\n",
            "|P43328-B21|   615.99|2024-10-01|    2498.0|\n",
            "|  AC830717|     NULL|2024-10-01|   1344.91|\n",
            "| 370-12345|     NULL|2024-10-01|   1399.28|\n",
            "|P64706-B21|   615.99|2024-10-01|    2498.0|\n",
            "|  AC239378|   825.99|2024-10-01|   1151.86|\n",
            "| 370-12344|     NULL|2024-10-01|   1399.28|\n",
            "+----------+---------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws,col\n",
        "\n",
        "product_details_df = products_df.join(attributes_df, on=\"PartNo\", how=\"inner\")\n",
        "\n",
        "key_columns = [\"company\", \"capacity\", \"rank\", \"speed\", \"technology\"]\n",
        "\n",
        "product_details_df = product_details_df.withColumn(\n",
        "    \"key\",\n",
        "    concat_ws(\"\", *[col(c) for c in key_columns])\n",
        ")\n",
        "\n",
        "product_details_df.orderBy(col(\"company\"), col(\"speed\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "TKjjjaJOXYrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Let's say we want to look at prices on 3 different dates:\n",
        "analysis_dates = [\n",
        "    {\"PartNo\": \"P43328-B21\", \"analysis_date\": \"2024-09-01\"},\n",
        "    {\"PartNo\": \"P43328-B21\", \"analysis_date\": \"2024-10-01\"},\n",
        "    {\"PartNo\": \"P43328-B21\", \"analysis_date\": \"2024-11-01\"},\n",
        "\n",
        "    {\"PartNo\": \"AC830717\", \"analysis_date\": \"2024-09-01\"},\n",
        "    {\"PartNo\": \"AC830717\", \"analysis_date\": \"2024-10-01\"},\n",
        "    {\"PartNo\": \"AC830717\", \"analysis_date\": \"2024-11-01\"},\n",
        "\n",
        "    {\"PartNo\": \"370-12345\", \"analysis_date\": \"2024-09-01\"},\n",
        "    {\"PartNo\": \"370-12345\", \"analysis_date\": \"2024-10-01\"},\n",
        "    {\"PartNo\": \"370-12345\", \"analysis_date\": \"2024-11-01\"},\n",
        "\n",
        "    {\"PartNo\": \"P64706-B21\", \"analysis_date\": \"2024-09-01\"},\n",
        "    {\"PartNo\": \"P64706-B21\", \"analysis_date\": \"2024-10-01\"},\n",
        "    {\"PartNo\": \"P64706-B21\", \"analysis_date\": \"2024-11-01\"},\n",
        "\n",
        "    {\"PartNo\": \"AC239378\", \"analysis_date\": \"2024-09-01\"},\n",
        "    {\"PartNo\": \"AC239378\", \"analysis_date\": \"2024-10-01\"},\n",
        "    {\"PartNo\": \"AC239378\", \"analysis_date\": \"2024-11-01\"},\n",
        "\n",
        "    {\"PartNo\": \"370-12344\", \"analysis_date\": \"2024-09-01\"},\n",
        "    {\"PartNo\": \"370-12344\", \"analysis_date\": \"2024-10-01\"},\n",
        "    {\"PartNo\": \"370-12344\", \"analysis_date\": \"2024-11-01\"},\n",
        "]\n",
        "\n",
        "analysis_dates_df = spark.createDataFrame(analysis_dates)\n",
        "\n",
        "# Join analysis_dates_df to base_prices_df to get baseline prices\n",
        "# Then join adjustments_df to get the adjustment factor for the analysis_date\n",
        "result_df = (\n",
        "    analysis_dates_df\n",
        "    # Join with base prices on PartNo, but the base price date is fixed (2024-10-01)\n",
        "    .join(base_prices_df, on=\"PartNo\", how=\"left\")\n",
        "    # Join adjustments on the analysis_date\n",
        "    .join(adjustments_df, analysis_dates_df[\"analysis_date\"] == adjustments_df[\"date\"], how=\"left\")\n",
        "    # Now we apply the (1 + adjustment_factor) to the base price\n",
        "    # Since baseline date = 2024-10-01, we consider that date's price as the reference\n",
        "    # If analysis_date == 2024-10-01, factor = 1 + 0.00 = 1.00 (no change)\n",
        "    # If analysis_date == 2024-09-01, factor = 1 - 0.05 = 0.95\n",
        "    # If analysis_date == 2024-11-01, factor = 1 + 0.07 = 1.07\n",
        "    .withColumn(\"final_list_price\", round(col(\"list_price\") * (1 + col(\"adjustment_factor\")), 2))\n",
        "    .withColumn(\"final_cdw_price\",\n",
        "                expr(\"CASE WHEN cdw_price IS NOT NULL THEN round(cdw_price * (1 + adjustment_factor), 2) ELSE NULL END\"))\n",
        "    .select(\"PartNo\", \"analysis_date\", \"list_price\", \"cdw_price\", \"adjustment_factor\", \"final_list_price\", \"final_cdw_price\")\n",
        ")\n",
        "\n",
        "result_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "3rHkykXBXe-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "# Extract HPE reference prices\n",
        "hpe_prices_df = (\n",
        "    result_df\n",
        "    .filter(col(\"PartNo\") == \"P43328-B21\")  # HPE product\n",
        "    .select(\n",
        "        col(\"analysis_date\").alias(\"ref_date\"),\n",
        "        col(\"final_list_price\").alias(\"hpe_final_list_price\"),\n",
        "        col(\"final_cdw_price\").alias(\"hpe_final_cdw_price\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract Dell Server reference price (370-12345)\n",
        "dell_server_prices_df = (\n",
        "    result_df\n",
        "    .filter(col(\"PartNo\") == \"370-12345\")  # Dell Server product\n",
        "    .select(\n",
        "        col(\"analysis_date\").alias(\"ref_date\"),\n",
        "        col(\"final_list_price\").alias(\"dell_server_list_price\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract Dell Upgrades price (AC830717)\n",
        "dell_upgrades_df = (\n",
        "    result_df\n",
        "    .filter(col(\"PartNo\") == \"AC830717\")  # Dell Upgrades product\n",
        "    .select(\n",
        "        col(\"analysis_date\"),\n",
        "        col(\"final_list_price\").alias(\"dell_upgrades_list_price\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Join Dell Upgrades with HPE references and Dell Server references by analysis_date\n",
        "comparison_df = (\n",
        "    dell_upgrades_df\n",
        "    .join(hpe_prices_df, dell_upgrades_df[\"analysis_date\"] == hpe_prices_df[\"ref_date\"], how=\"left\")\n",
        "    .join(dell_server_prices_df, dell_upgrades_df[\"analysis_date\"] == dell_server_prices_df[\"ref_date\"], how=\"left\")\n",
        "    .select(\n",
        "        dell_upgrades_df[\"analysis_date\"],\n",
        "        \"dell_upgrades_list_price\",\n",
        "        \"hpe_final_list_price\",\n",
        "        \"hpe_final_cdw_price\",\n",
        "        \"dell_server_list_price\"\n",
        "    )\n",
        "    # Calculate the requested ratios\n",
        "    .withColumn(\"ratio_dell_upgrades_vs_dell_server\", expr(\"dell_upgrades_list_price / dell_server_list_price\"))\n",
        "    .withColumn(\"ratio_dell_upgrades_vs_hpe\", expr(\"dell_upgrades_list_price / hpe_final_list_price\"))\n",
        "    .withColumn(\"ratio_dell_upgrades_vs_cdw\", expr(\"dell_upgrades_list_price / hpe_final_cdw_price\"))\n",
        ")\n",
        "\n",
        "comparison_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "CB2Tb4xnXwc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New aproach, to match parameters, more complex scenario."
      ],
      "metadata": {
        "id": "Lru9Mw7YiMrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "# Assume we already have:\n",
        "# products_df, attributes_df, base_prices_df, adjustments_df, analysis_dates_df\n",
        "# and a final result_df with final_list_price and final_cdw_price after applying date-based adjustments.\n",
        "\n",
        "# 1. Enrich result_df with attributes and product company\n",
        "enriched_df = (\n",
        "    result_df\n",
        "    .join(attributes_df, on=\"PartNo\", how=\"left\")       # Add capacity, speed, rank, technology\n",
        "    .join(products_df, on=\"PartNo\", how=\"left\")         # Add company, description\n",
        "    .select(\n",
        "        \"PartNo\",\n",
        "        \"company\",\n",
        "        \"capacity\",\n",
        "        \"speed\",\n",
        "        \"rank\",\n",
        "        \"technology\",\n",
        "        \"analysis_date\",\n",
        "        \"final_list_price\",\n",
        "        \"final_cdw_price\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Now we have a DataFrame that includes company and attributes for each product at each analysis_date.\n",
        "\n",
        "# 2. Create separate DataFrames for each company category:\n",
        "hpe_df = (\n",
        "    enriched_df\n",
        "    .filter(col(\"company\") == \"HPE\")\n",
        "    .select(\"capacity\", \"speed\", \"rank\", \"technology\", \"analysis_date\",\n",
        "            col(\"final_list_price\").alias(\"hpe_final_list_price\"),\n",
        "            col(\"final_cdw_price\").alias(\"hpe_final_cdw_price\"))\n",
        ")\n",
        "\n",
        "dell_server_df = (\n",
        "    enriched_df\n",
        "    .filter(col(\"company\") == \"DELLServer\")\n",
        "    .select(\"capacity\", \"speed\", \"rank\", \"technology\", \"analysis_date\",\n",
        "            col(\"final_list_price\").alias(\"dell_server_list_price\"))\n",
        ")\n",
        "\n",
        "dell_upgrades_df = (\n",
        "    enriched_df\n",
        "    .filter(col(\"company\") == \"DELLUpgrades\")\n",
        "    .select(\"capacity\", \"speed\", \"rank\", \"technology\", \"analysis_date\",\n",
        "            col(\"final_list_price\").alias(\"dell_upgrades_list_price\"))\n",
        ")\n",
        "\n",
        "# 3. Join these three DataFrames on the attribute set plus the analysis_date\n",
        "comparison_df = (\n",
        "    dell_upgrades_df\n",
        "    .join(hpe_df, on=[\"capacity\", \"speed\", \"rank\", \"technology\", \"analysis_date\"], how=\"left\")\n",
        "    .join(dell_server_df, on=[\"capacity\", \"speed\", \"rank\", \"technology\", \"analysis_date\"], how=\"left\")\n",
        "    .select(\n",
        "        \"capacity\", \"speed\", \"rank\", \"technology\", \"analysis_date\",\n",
        "        \"dell_upgrades_list_price\",\n",
        "        \"hpe_final_list_price\",\n",
        "        \"hpe_final_cdw_price\",\n",
        "        \"dell_server_list_price\"\n",
        "    )\n",
        "    # Calculate ratios based on the matched attributes\n",
        "    .withColumn(\"ratio_dell_upgrades_vs_dell_server\", expr(\"dell_upgrades_list_price / dell_server_list_price\"))\n",
        "    .withColumn(\"ratio_dell_upgrades_vs_hpe\", expr(\"dell_upgrades_list_price / hpe_final_list_price\"))\n",
        "    .withColumn(\"ratio_dell_upgrades_vs_cdw\", expr(\"dell_upgrades_list_price / hpe_final_cdw_price\"))\n",
        ")\n",
        "\n",
        "comparison_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "hKSzKzXbZXwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df.printSchema()"
      ],
      "metadata": {
        "id": "kc400krenbtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# If analysis_date is a string, make sure it's properly parsed to a date in Spark:\n",
        "# (If it's already a proper date type, you can skip this step)\n",
        "comparison_df = comparison_df.withColumn(\"analysis_date\", to_date(\"analysis_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "pdf = comparison_df.select(\"analysis_date\", \"dell_upgrades_list_price\", \"dell_server_list_price\").toPandas()\n",
        "\n",
        "# Ensure that analysis_date is a datetime in Pandas (if needed)\n",
        "pdf[\"analysis_date\"] = pd.to_datetime(pdf[\"analysis_date\"])\n",
        "\n",
        "pdf\n"
      ],
      "metadata": {
        "id": "w2hI81R5iWNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the 'analysis_date' as the index if desired (not mandatory)\n",
        "pdf.set_index(\"analysis_date\", inplace=True)\n",
        "\n",
        "pdf"
      ],
      "metadata": {
        "id": "Wmm6XtNdpQwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot using seaborn or matplotlib\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.lineplot(data=pdf[[\"dell_upgrades_list_price\", \"dell_server_list_price\"]], errorbar=None, linewidth=2.0)\n",
        "\n",
        "\n",
        "# Start y-axis from zero\n",
        "plt.ylim(0, None)\n",
        "\n",
        "plt.title(\"Dell Upgrades vs Dell Server Price Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D_5MUO4hjpXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# If analysis_date is a string, make sure it's properly parsed to a date in Spark:\n",
        "# (If it's already a proper date type, you can skip this step)\n",
        "comparison_df = comparison_df.withColumn(\"analysis_date\", to_date(\"analysis_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "pdf = comparison_df.select(\"analysis_date\", \"dell_upgrades_list_price\", \"dell_server_list_price\").toPandas()\n",
        "\n",
        "# Suppose pdf is your Pandas DataFrame with columns \"analysis_date\", \"dell_upgrades_list_price\", and \"dell_server_list_price\"\n",
        "pdf[\"analysis_date\"] = pd.to_datetime(pdf[\"analysis_date\"])\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Plot without aggregation, no confidence interval, and show points\n",
        "sns.lineplot(data=pdf, x=\"analysis_date\", y=\"dell_upgrades_list_price\", ci=None, estimator=None, marker='o', label=\"Dell Upgrades\")\n",
        "sns.lineplot(data=pdf, x=\"analysis_date\", y=\"dell_server_list_price\", ci=None, estimator=None, marker='o', label=\"Dell Server\")\n",
        "\n",
        "# Format the x-axis to show only the given dates\n",
        "unique_dates = pdf[\"analysis_date\"].unique()\n",
        "plt.xticks(unique_dates, [d.strftime(\"%Y-%m-%d\") for d in unique_dates])\n",
        "\n",
        "# Start y-axis from zero\n",
        "plt.ylim(0, None)\n",
        "\n",
        "plt.title(\"Dell Upgrades vs Dell Server Price Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5VmaESQEt9Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOpGliZcvJRx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}